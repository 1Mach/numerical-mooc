{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under MIT license (c)2015 L.A. Barba, G.F. Forsyth, P.Y. Chuang. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relax and hold steady"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the fourth notebook of *Relax and hold steady: elliptic problems*, the fifth module of [**\"Practical Numerical Methods with Python\"**](http://openedx.seas.gwu.edu/courses/GW/MAE6286/2014_fall/about).\n",
    "\n",
    "In the previous notebooks, we examined traditional relaxation methods (iterative methods).  Modern engineering problems involve millions of grid points and as the number of grid points grows, the convergence rate of iterative methods starts to suffer.  Enter multigrid.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multigrid basics: convergence of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the discretized 1D Poisson equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{p_{i+1}-2p_i+p_{i-1}}{\\Delta x^2}=b_i ,\\ \\ \\ \\ \\ 0 \\le i \\le N_x-1\n",
    "\\end{equation}\n",
    "\n",
    "where $N_x$ is the number of grid points. Since the indices of the grid points start from $0$, the index of the last point is $N_x-1$. We start with Dirichlet boundary conditions: $$p_0=p_{N_x-1}= 0$$\n",
    "\n",
    "If $p{^k_i}$ and $p{^{exact}_i}$ denote the approximated solution at the **n-th iteration** in a relaxation method and the exact solution, respectively, we can define the error at this iteration as\n",
    "\n",
    "\\begin{equation}\n",
    "e{^k_i} = p{^{exact}_i} - p{^k_i} ,\\ \\ \\ \\ \\ 0 \\le i \\le N_x-1\n",
    "\\end{equation}\n",
    "\n",
    "That is, the error is the difference between the exact and the estimated solution.  \n",
    "\n",
    "When we specify an initial guess, $p{^0_i}$, the initial error will be given by \n",
    "\n",
    "$$e{^0_i} = p{^{exact}_i} - p{^0_i}$$\n",
    "\n",
    "When we solve an elliptic PDE by any iterative method we try to eliminate this error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to apply a Fourier transform to the errors $e^k_i$, we would find that the errors are actually composed of many different single-wavenumber waves. This yields an interesting question: \n",
    "\n",
    "When we eliminate errors using a relaxation method, do these wavenumbers make an impact on the convergence rate? \n",
    "\n",
    "We can answer this with some simple tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavenumbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, consider the 1D Laplace equation with Dirichlet boundaries\n",
    "\n",
    "$$p_0=p_{N_x-1}= 0$$\n",
    "\n",
    "The exact solution is $p{^{exact}_i}=0$ and Equation $()$ now simplifies to \n",
    "\n",
    "\\begin{equation}\n",
    "e{^k_i}=-p{^k_i}\n",
    "\\end{equation}\n",
    "\n",
    "We want to examine error convergence as a function of wavenumber so we need a function that is dependent on wavenumber.  We can define $p^0_i$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "p{^0_i} = \\sin{\\left(\\frac{ik\\pi}{N_x-1} \\right)} ,\\ \\ \\ \\ \\ 0 \\le i \\le N_x-1\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is the wavenumber.  Let's take a look at our initial condition for a few different values of $k$.  \n",
    "\n",
    "<!---\n",
    "\n",
    "In order to check the convergence of errors with different wavenumbers, we use several different single-wavenumber waves as our initial error (i.e. initial guess in our current example). Thus the initial guess is\n",
    "\n",
    "\\begin{equation}\n",
    "p{^0_i} = \\sin{(\\frac{ik\\pi}{N_x-1})} ,\\ \\ \\ \\ \\ 0 \\le i \\le N_x-1\n",
    "\\end{equation}\n",
    "\n",
    "$k$ is the wavenumber. Five different $k$ are tested: 1) $k=1$, 2) $k=16$, 3) $k=31$, 4) $k=48$, and 5) $k=63$. We also carry out the tests using two different $N_x$: $N_x=65$ and $N_x=33$. In each case, a total of $100$ iterations will be executed. The selected relaxation method is the Gauss-Seidel method.\n",
    "\n",
    "First, we have to define some frequently used functions.\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot, rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_values = [1, 16, 31, 48, 63]\n",
    "nx = 65\n",
    "i = numpy.arange(nx-1)\n",
    "\n",
    "pyplot.figure(figsize=(10,6))\n",
    "\n",
    "for j,k in enumerate(k_values):\n",
    "    pyplot.subplot(2,3,j+1)\n",
    "    p = numpy.sin(i * k * numpy.pi / (nx-1))\n",
    "    pyplot.plot(numpy.linspace(0,1,nx-1), p)\n",
    "    pyplot.title(\"$k = $\"+str(k));\n",
    "pyplot.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are certainly different!  Now we want to investigate how the error decreases for each of these different initial conditions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few functions to help streamline the error analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def laplace1d_IC(nx, k):\n",
    "    '''\n",
    "    Generates initial guess for Laplace 1D eq. under a given number \n",
    "    of grid points (nx) and wavenumber k within domain [0,1]x[0,1]\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    nx: int, number of grid points in x direction\n",
    "    k:  float, wavenumber    \n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    p: 1D array of float, initial guess of the unknowns\n",
    "    b: 1D array of float, 0th-order derivative term in Poisson eq.\n",
    "    x: 1D array of float, linspace coordinates in x\n",
    "    dx: float, grid spacing in x\n",
    "    '''\n",
    "    \n",
    "    dx = 1.0/(nx-1)\n",
    "    x = numpy.linspace(0,1,nx)\n",
    "    \n",
    "    ##initial conditions\n",
    "    i = numpy.arange(0, nx)\n",
    "    p = numpy.sin(i * k * numpy.pi / (nx-1))\n",
    "    b = numpy.zeros(nx)\n",
    "    \n",
    "    return p, b, x, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve our example problem we'll use Gauss-Seidel and accelerate it with Numba.  You will notice that in contrast to the previous notebook, the Poisson and Laplace solving functions perform only a single iteration per function call.  The reason for this will become clear as we delve deeper into multigrid methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import autojit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@autojit(nopython=True)\n",
    "def poisson1d_GS_SingleItr(nx, dx, p, b):\n",
    "    '''\n",
    "    Gauss-Seidel method for 1D Poisson eq. with Dirichlet BCs at both \n",
    "    ends. Only a single iteration is executed. **blitz** is used.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    nx: int, number of grid points in x direction\n",
    "    dx: float, grid spacing in x\n",
    "    p: 1D array of float, approximated soln. in last iteration\n",
    "    b: 1D array of float, 0th-order derivative term in Poisson eq.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    p: 1D array of float, approximated soln. in current iteration\n",
    "    '''\n",
    "    for i in range(1,len(p)-1):\n",
    "        p[i] = 0.5 * (p[i+1] + p[i-1] - dx**2 * b[i])\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a relative $L_2$-norm to calculate the error in the previous notebooks, but that doesn't work very well when the exact solution is a line of zeros.  Instead of $L_2$, we can use the *root-mean-square* of the difference of two solutions to compare error between different grid sizes in a meaningful way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RMS(p):\n",
    "    '''\n",
    "    Return the root mean square of p.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    p:   array\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    Root mean square of p\n",
    "    '''\n",
    "    return numpy.sqrt(numpy.sum(p**2) / p.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get started!  We will examine the error associated with the wavenumbers\n",
    "\n",
    "$$k = [1, 16, 31, 48, 63 ]$$\n",
    "\n",
    "for two different grid sizes, \n",
    "\n",
    "$$nx = [65, 33]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We are storing the error for each wavenumber-grid combination in a [dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries).  A dictionary is an unordered collection with *keys* and *values*.  Each key in our dictionary is a [tuple](https://docs.python.org/2/tutorial/datastructures.html#tuples-and-sequences) comprised of the grid-size and the wavenumber.  The corresponding value for a given key (in the code below) is a NumPy array containing the RMS error for each iteration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define different Np\n",
    "nx = [65, 33]\n",
    "# define wavenumbers\n",
    "k = [1, 16, 31, 48, 63]\n",
    "# number of iterations we will run\n",
    "Nitr = 100\n",
    "# initialize a space to store root mean square errors\n",
    "err = {}\n",
    "\n",
    "# start iteration\n",
    "for nxi in nx:\n",
    "    for ki in k:\n",
    "        key = (nxi, ki)\n",
    "        err[key] = numpy.empty(Nitr+1)\n",
    "        p, b, x, dx = laplace1d_IC(nxi, ki)\n",
    "        err[key][0] = RMS(p)\n",
    "        for itr in range(1, Nitr+1):\n",
    "            p = poisson1d_GS_SingleItr(nxi, dx, p, b)\n",
    "            err[key][itr] = RMS(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot errors in each case and at each iteration\n",
    "pyplot.figure(figsize=(9, 4.1))\n",
    "for n, nxi in enumerate(nx):\n",
    "    pyplot.subplot(1, 2, n+1)\n",
    "    for ki in k:\n",
    "        pyplot.semilogy(numpy.array(range(Nitr+1)), err[(nxi, ki)], \n",
    "                        lw=2, label='k='+str(ki))\n",
    "\n",
    "        pyplot.legend(loc=3, fontsize='small', ncol=3, mode='expand')\n",
    "        pyplot.xlabel('Iteration') \n",
    "        pyplot.ylabel('RMS Error')\n",
    "        pyplot.title('Nx = '+str(nxi))\n",
    "        pyplot.ylim((1e-8, 1))\n",
    "pyplot.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aliasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there only three lines in the $N_x=33$ plot?  In fact, there are five lines, but two of them are hiding.  What's happening?\n",
    "\n",
    "Any grid has a limited resolution and if we try to resolve a rapidly oscillating function on a coarser grid, we encounter a phenomenon called [aliasing](http://nbviewer.ipython.org/github/ketch/teaching-numerics-with-notebooks/blob/master/Aliasing.ipynb).  The high wavenumber error can't be fully resolved in the domain, so it wraps around.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the error for wavenumbers $k = [48, 63]$ is exactly the same as for wavenumbers $k=[16, 1]$, respectively.  That's a visual observation, but we also check the numbers using the error dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy.allclose(err[(33,1)],err[(33,63)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy.allclose(err[(33,16)],err[(33,48)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implication here is that no error can have a wavenumber greater than $N_x-1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important observation in the plots above is the rate of convergence.  \n",
    "\n",
    "Compare the rate of convergence for wavenumbers $k=[1,16,31]$ in both plots above.  What do you see?  The error converges more quickly for these wavenumbers in the $N_x=33$ plot.  \n",
    "\n",
    "But what about the wavenumbers $k = [48, 63]$?  They clearly converge faster in the $N_x=65$ domain.  \n",
    "\n",
    "These observations are at the heart of multigrid.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---We call the errors with wavenumbers smaller than $(N_x-1)/2$ low-wavenumber errors; the errors with wavenumbers larger than $(N_x-1)/2$ are high-wavenumber errors.\n",
    "\n",
    "If we have a grid with $N_x$ points on it, we can first use this grid to eliminate high-wavenumber errors, then transfer low-wavenumber errors to another grid with $(N_x+1)/2$ points to accelerate convergence.  --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multigrid tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that a solution exists to the Poisson equation\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla^2 p = b\n",
    "\\end{equation}\n",
    "\n",
    "then we know that at each point in the domain, there will be some error between the calculated value, $p^k_i$ and the exact solution $p^{exact}_i$.  We may not know what the exact solution is, but we know it's out there.  Earlier, we defined the error at any point $i$ as\n",
    "\n",
    "\\begin{equation}\n",
    "e^k_i = p^{exact}_i - p^k_i\n",
    "\\end{equation}\n",
    "\n",
    "**Note:** We are talking about error at a specific point, not a measure of error across the entire domain.  \n",
    "\n",
    "What if we recast the Poisson equation in terms of a not-perfectly-relaxed $p^k$?\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla^2 p^k \\approx b\n",
    "\\end{equation}\n",
    "\n",
    "Equation $(7)$ is an approximation because $p^k \\neq p$.  To \"fix\" the equation, we need to add an extra term to account for the difference in the Poisson equation -- that extra term is called the residual.  We can write out the modified Poisson equation like this:\n",
    "\n",
    "\\begin{equation}\n",
    "r^k + \\nabla^2 p^k = b\n",
    "\\end{equation}\n",
    "\n",
    "Rearranging and discretizing with 2nd-order central difference yields \n",
    "\n",
    "\\begin{equation}\n",
    "r^k_i = b_i - \\frac{p{^k_{i+1}}-2p{^k_i}+p{^k_{i-1}}}{\\Delta x^2}\n",
    "\\end{equation}\n",
    "\n",
    "If we combine Equation (6) with Equation (9), it gives us the following result:\n",
    "\n",
    "\\begin{equation}\n",
    "r^k_i = \\frac{e{^k_{i+1}}-2e{^k_i}+e{^k_{i-1}}}{\\Delta x^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still with us?  What sort of system does Equation $(10)$ look like?  The residual can be used as a source term to *relax* the **error** itself.  Then, according to Equation $(6)$, we can add this relaxed error back to the corresponding $p_i$ term to increase the accuracy of the solution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def residual(dx, pn, b, r):\n",
    "    '''\n",
    "    Calculate the residual for the 1D Poisson equation.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    pn: 1D array, approximated solution at a certain iteration n\n",
    "    b:  1D array, the b(x) in the Poisson eq.\n",
    "    \n",
    "    Return:\n",
    "    ----------\n",
    "    The residual r\n",
    "    '''\n",
    "    \n",
    "    # r[0] = 0\n",
    "    r[1:-1] = b[1:-1] - (pn[:-2] - 2 * pn[1:-1] + pn[2:]) / dx**2\n",
    "    # r[-1] = 0\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important bookkeeping operations in multigrid is the transfer of data from one grid to another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restriction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfering data from a fine grid to a coarse grid is called restriction. There are many ways to implement restriction. Here we introduce two restriction operators. The first is called **injection**, the other is **full weighting**.\n",
    "\n",
    "**Injection**  \n",
    "Injection is the simplest method of restriction.  Where two grids have matching coordinates, it copies over the corresponding data.  In the locations where one grid has points and the other doesn't, it simply ignores that data.  \n",
    "\n",
    "If we have a coarse grid with half as many points as a fine grid, we can write out the injection operator as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "v_{i, c} = v_{2i, f}\n",
    "\\end{equation}\n",
    " \n",
    "With Dirichlet boundary conditions, the boundaries are mapped one-to-one.  Neumann boundary conditions require a little extra consideration as we'll see later.  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/injection.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1. Injection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full weighting**  \n",
    "In contrast to injection, full weighting considers the contributions of all points when mapping from the fine grid to the coarse grid.  \n",
    "\n",
    "\\begin{equation}\n",
    "v_{i,c}=\\frac{1}{4}\\left(v_{2i-1,f}+2v_{2i,f}+v_{2i+1,f}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The Dirichlet boundary conditions are again mapped one-to-one.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/full_weighting.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2. Full Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dig deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different restriction operators can impact the rate of convergence.  We implement full weighting in this example but you should also try the same problem using injection and compare the performance of the two methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows a function for full weighting. Note that we use **stepped slicing** here. \n",
    "\n",
    "The syntax for stepped slicing is\n",
    "```Python\n",
    "array[start:end:stepsize]\n",
    "```\n",
    "For example, `x[1:10:2]` would return the values of `x[1]`, `x[3]`, `x[5]`, `x[7]`, and `x[9]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def full_weighting_1d(vF, vC):\n",
    "    '''\n",
    "    Transfer a vector on a fine grid to a coarse grid with full weighting \n",
    "    .  The number of elements (not points) of the coarse grid is \n",
    "    half of that of the fine grid.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    vF: 1D numpy array, the vector on the fine grid\n",
    "    vC: 1D numpy array, the vector on the coarse grid,\n",
    "        size(vC) = (size(vF) + 1) / 2\n",
    "    \n",
    "    Output: vC\n",
    "    '''\n",
    "    \n",
    "    vC[0] = vF[0]\n",
    "    vC[1:-1] = 0.25 * (vF[1:-3:2] + 2. * vF[2:-2:2] + vF[3:-1:2])\n",
    "    vC[-1] = vF[-1]\n",
    "    \n",
    "    return vC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation or prolongation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we relax the residual equation on the coarse grid, we need an operation that can transfer the relaxed errors back to the fine grid.\n",
    "\n",
    "The simplest way to move from the coarse grid to the fine grid is **linear interpolation**.  The values of the points common to both grids are set equal while the values at the remaining points on the fine grid are calculated via linear interpolation, as shown below.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "v_{2i,f}=v_{i, c} ,\\ \\ \\ \\ \\ 0 \\le i \\le N_{x,c}-1  \\\\\n",
    "v_{2i+1,f}=\\frac{1}{2}\\left(v_{i,c}+v_{i+1,c}\\right) ,\\ \\ \\ \\ \\ 0 \\le i \\le N_{x,c}-2\n",
    "\\end{array}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interpolation_1d(vC, vF):\n",
    "    '''\n",
    "    Transfer a vector on a coarse grid to a fine grid by linear \n",
    "    interpolation. The number of elements (not points) of the coarse \n",
    "    grid is a half of that of the fine grid.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    vC: 1D numpy array, the vector on the coarse grid,\n",
    "    vF: 1D numpy array, the vector on the fine grid\n",
    "        size(vF) = size(vC) * 2 - 1\n",
    "    \n",
    "    Output: vF\n",
    "    '''\n",
    "    \n",
    "    vF[::2] = vC[:]\n",
    "    vF[1:-1:2] = 0.5 * (vC[:-1] + vC[1:])\n",
    "    \n",
    "    return vF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. David Ketcheson, Aron Ahmadia, [Aliasing](http://nbviewer.ipython.org/github/ketch/teaching-numerics-with-notebooks/blob/master/Aliasing.ipynb)  from [*Teaching Numerics with Notebooks*](https://github.com/ketch/teaching-numerics-with-notebooks), 2014\n",
    "2. William L. Griggs, Van Emden Henson, and Steve F. McCormick, [*A Multigrid Tutorial*](https://play.google.com/store/books/details/William_L_Briggs_A_Multigrid_Tutorial?id=ahklDynzz8cC), SIAM, Philadephia, 2000 \n",
    "3. U. Trottenberg, C. W. Oosterlee, Anton Schüller, [*Multigrid*](https://play.google.com/store/books/details/Ulrich_Trottenberg_Multigrid?id=9ysyNPZoR24C), Academic press, 2000\n",
    "\n",
    "4. Jonathan Shewchuk, [*An Introduction to the Conjugate Gradient Method without the Agonizing Pain*](http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf), 1994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "css_file = '../../styles/numericalmoocstyle.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
