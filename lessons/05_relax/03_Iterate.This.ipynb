{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under MIT license (c)2014 L.A. Barba, C.D. Cooper, G.F. Forsyth.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate This!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Lesson 1](./2D_Laplace_Equation.ipynb) and [Lesson 2](./2D_Poisson_Equation.ipynb) of this module we used the Jacobi method to iteratively solve for solutions to elliptic PDEs.  \n",
    "\n",
    "And it worked, so why are we still talking about it?  Because it's slow.  Very, very slow.  It might not have seemed that way in the first two notebooks because our domains were quite tiny, but consider this:  for a domain with $nx = ny = 128$, the Jacobi method will require nearly *20000* iterations before the steady state check reaches $10^{-8}$.\n",
    "\n",
    "Using one core of an Intel i7 3.5 GHz processor, that takes around 3 seconds.  Now consider this:  an incompressible Navier Stokes solver has to ensure that the pressure field is divergence-free at every timestep.  One of the most common ways to ensure that the pressure field is divergence-free is to *relax* the pressure field using *iterative methods*!  \n",
    "\n",
    "In fact, the pressure condition is responsible for the majority of the computational expense of a Navier Stokes solver.  And with the current set up, each timestep could require 3 seconds of CPU time just to satisfy the pressure constraints!  We'll grow old and die before we ever get cavity flow working!\n",
    "\n",
    "There has to be a better way.  And, of course, there is.  There are several!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the same example problem that we covered in [Lesson 1](./2D_Laplace_Equation.ipynb), with boundary conditions\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{gathered}\n",
    "p=0 \\text{ at } x=0\\\\\n",
    "\\frac{\\partial p}{\\partial x} = 0 \\text{ at } x = L\\\\\n",
    "p = 0 \\text{ at }y = 0 \\\\\n",
    "p = \\sin \\left(  \\frac{\\frac{3}{2}\\pi x}{L} \\right) \\text{ at } y = H\n",
    "  \\end{gathered}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of copying and pasting cells that we wrote in [Lesson 1](), we have again created a 'helper' file that you can import some useful functions from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from laplace_helper import p_analytical, plot2D, L2_rel_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the `p_analytical`, `plot2D`, and `L2_rel_error` functions in our namespace.  If you can't remember how they work, just use `help()` and take advantage of the docstrings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use larger grid dimensions in this notebook to better illustrate the speed increases available with different iterative methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx = 128\n",
    "ny = 128\n",
    "\n",
    "L = 5\n",
    "H = 5\n",
    "\n",
    "x = numpy.linspace(0,L,nx)\n",
    "y = numpy.linspace(0,H,ny)\n",
    "\n",
    "dx = L/(nx-1)\n",
    "dy = H/(ny-1)\n",
    "\n",
    "p0 = numpy.zeros((ny, nx))\n",
    "\n",
    "p0[-1,:] = numpy.sin(1.5*numpy.pi*x/x[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first -- we said above that the Jacobi method takes nearly 20000 iterations before it satisfies the steady state target L2-norm of $10^{-8}$, but let's verify that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def laplace2d(p, y, dx, dy, l2_target):\n",
    "    '''Solves the diffusion equation with forward-time, centered scheme\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    p: 2D array of float\n",
    "        Initial potential distribution\n",
    "    y: array of float\n",
    "        Nodal coordinates in y\n",
    "    dx: float\n",
    "        Mesh size\n",
    "    dy: float\n",
    "        Mesh size\n",
    "    l2_target: float\n",
    "        Error target\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    p: 2D array of float\n",
    "        Potential distribution after relaxation\n",
    "    '''\n",
    "    \n",
    "    l2norm = 1\n",
    "    pn = numpy.empty_like(p)\n",
    "    iterations = 0\n",
    "\n",
    "    while l2norm > l2_target:\n",
    "        pn = p.copy()\n",
    "        p[1:-1,1:-1] = .25 * (pn[1:-1,2:] + pn[1:-1,:-2] +\\\n",
    "                              pn[2:,1:-1] + pn[:-2,1:-1])\n",
    "        \n",
    "        ##Neumann B.C. along x = L\n",
    "        p[1:-1,-1] = .25 * (2*pn[1:-1,-2] + pn[2:,-1] + pn[:-2, -1])\n",
    "        \n",
    "        l2norm = numpy.sqrt(numpy.sum((p - pn)**2)/numpy.sum(pn**2))\n",
    "        iterations += 1\n",
    "     \n",
    "    return p, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "p, iterations = laplace2d(p0.copy(), y, dx, dy, eps)\n",
    "\n",
    "print (\"Jacobi method took {} iterations at tolerance {}\".\\\n",
    "        format(iterations, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would we lie to you?  19993 iterations before the steady state check dips below $10^{-8}$.  \n",
    "\n",
    "We can also time how long the Jacobi method takes using the `%%timeit` cell-magic.  This can take a little bit of time -- the `%%timeit` magic runs the function a few times and then averages their runtimes to give a more accurate result.  \n",
    "\n",
    "<br>\n",
    "<div class=\"alert alert-success\">\n",
    "Note also that when using `%%timeit`, the return values of a function (`p` and `iterations` in this case) *won't* be saved.\n",
    "</div>\n",
    "\n",
    "With those caveats, let's give it a shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "laplace2d(p0.copy(), y, dx, dy, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little over 3 seconds.  Not terrible, but not great.  First let's check to see if it's even accurate by comparing it to the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pan = p_analytical(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L2_rel_error(p,pan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's a pretty small error.  Let's focus on speeding up the process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Seidel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will recall from [Lesson 1](./2D_Laplace_Equation.ipynb) that a single Jacobi iteration is written as:\n",
    "\n",
    "\\begin{equation}\n",
    "p^{n+1}_{i,j} = \\frac{1}{4} \\left(p^{n}_{i,j-1} + p^n_{i,j+1} + p^{n}_{i-1,j} + p^n_{i+1,j} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "The Gauss-Seidel method is a simple tweak to this idea -- use updated values as soon as they are available.  \n",
    "\n",
    "If you imagine that we progress through an array in the following order:\n",
    "\n",
    "<img src=\"./figures/solvepath.svg\" width=350>\n",
    "\n",
    "\n",
    "Then you can see that the values $p^{n+1}_{i-1,j}$ and $p^{n+1}_{i,j-1}$ can be used to calculate $p^{n+1}_{i,j}$, so the iteration formula will now read as:\n",
    "\n",
    "\\begin{equation}\n",
    "p^{n+1}_{i,j} = \\frac{1}{4} \\left(p^{n+1}_{i,j-1} + p^n_{i,j+1} + p^{n+1}_{i-1,j} + p^n_{i+1,j} \\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's a problem.  You can't use NumPy's array operations to evaluate that.  Since Gauss-Seidel requires using values immediately after they're updated, we have to abandon our beloved array operations and return to nested `for` loops.  \n",
    "\n",
    "That's not ideal, but if it saves us a bunch of time, then we can manage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def laplace2d_gauss_seidel(p, y, dx, dy, nx, ny, eps):\n",
    "    \n",
    "    iterations = 0\n",
    "    error = 2*eps\n",
    "    \n",
    "    while error > eps:\n",
    "        pn = p.copy()\n",
    "        error = 0.0\n",
    "        for j in range(1,ny-1):\n",
    "            for i in range(1,nx-1):\n",
    "                p[j,i] = .25 * (p[j,i-1] + p[j,i+1] + p[j-1,i] + p[j+1,i])\n",
    "                error += (p[j,i] - pn[j,i])**2\n",
    "        \n",
    "        #Neumann 2nd-order BC\n",
    "        for j in range(1,ny-1):\n",
    "            p[j,-1] = .25 * (2*p[j,-2] + p[j+1,-1] + p[j-1, -1])\n",
    "            \n",
    "        error = numpy.sqrt(error/numpy.sum(pn**2))\n",
    "        iterations += 1        \n",
    "        \n",
    "    return p, iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we would run this via:\n",
    "\n",
    "```Python\n",
    "p, iterations = laplace2d_gauss_seidel(p, y, dx, dy, nx, ny, 1e-8)\n",
    "```\n",
    "\n",
    "<br>\n",
    "But **don't do it**.  We did it so that you don't have to.  \n",
    "\n",
    "The Gauss-Seidel method required several thousand fewer iterations than the traditional Jacobi method for this example, but it took more than *7 minutes* to run.  \n",
    "\n",
    "And we were complaining about 3 seconds!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think back to the far off days when you first learned about array operations, you might recall that we discovered that NumPy array operations could drastically improve code performance compared with nested `for` loops.  NumPy operations are largely written in compiled C code and they are *much* faster than vanilla Python.  But the Jacobi method is old and while 3 seconds is much better than 7 minutes, it's still too slow.  What can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Numba!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba is a Just-In-Time (JIT) compiler for Python.  It takes a chunk of Python, converts it into optimized machine code, then compiles it and runs it.  \n",
    "\n",
    "It can massively speed up performance, especially when dealing with loops.  Plus, it's pretty easy to use!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h5>Caveat</h5>\n",
    "<br>\n",
    "\n",
    "We encourage everyone following the course to use the Anaconda Python distribution because it's well put-together and simple to use.  If you *haven't* been using Anaconda, that's fine, but let us **strongly** suggest that you take the plunge now.  Numba is great and easy to use, but it is **not** easy to install without help.  Those of you using Anaconda can install it by running <br><br>\n",
    "\n",
    "`conda install numba`<br><br>\n",
    "\n",
    "If you *really* don't want to use Anaconda, you can find instructions on how to compile Numba's dependencies [here](https://pypi.python.org/pypi/numba)\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to dive in!  Numba is great and easy to use.  We're going to walk you through a simple example first to give you a taste of Numba's abilities.  \n",
    "\n",
    "First, let's import `numba` and also import `autojit` (more on this in a bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import autojit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You tell Numba which functions you want to accelerate by using Python decorators.  Let's write a quick function that calculates the $n$th number in the Fibonacci sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fibit(n):\n",
    "    a = 1\n",
    "    b = 1\n",
    "    for i in range(n-2):\n",
    "        a, b = b, a+b\n",
    "        \n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several faster ways to program the Fibonacci sequence, but that's not a concern right now (but if you're curious, check [this](http://mathworld.wolfram.com/BinetsFibonacciNumberFormula.html) out).  Let's use `%%timeit` and see how long this simple function takes to find the 500,000th Fibonacci number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "fibit(500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 2.5 seconds on one of our machines.  Now let's try Numba!  Just add the `@autojit` decorator above the function name and let's see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@autojit\n",
    "def fibit(n):\n",
    "    a = 1\n",
    "    b = 1\n",
    "    for i in range(n-2):\n",
    "        a, b = b, a+b\n",
    "        \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "fibit(500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy cow!  That warning from `%%timeit` is due to the compilation overhead for Numba.  The very first time that it runs, it has to compile the generated machine code, then it caches that code for reuse without extra compiling.  That's the 'Just=In-Time' bit.  You'll see it disappear if we run `%%timeit` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "fibit(500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes this is a slightly contrived example, but that's a 10000x increase in speed!  For one line!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nopython` mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba is very clever, but it can't optimize everything.  When it can't, rather than failing to run, it will fall back to the regular Vanilla Python that we decided is far too slow.  This can be frustrating, since you might not know which bits of code will speed up and which bits won't.  \n",
    "\n",
    "To avoid this particular annoyance, you can tell Numba to use `nopython` mode.  This will raise an exception if there is code that can't be optimized in a function.  \n",
    "\n",
    "One quick example -- a great deal of Numpy functionality is available within Numba accelerated functions, but not everything is there.  Consider the function below that makes a copy of an array and then adds one to every value.  It's a silly function, but it will run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def increment_array(x):\n",
    "    a = x.copy()\n",
    "    a += 1\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = numpy.random.random(10)\n",
    "a = increment_array(x)\n",
    "print(x)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in-place addition isn't supported for arrays within Numba using `nopython` mode.  It will still compile and run, but it might cause a slowdown.  Instead, we can tell Numba to use `nopython` mode.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@autojit(nopython=True)\n",
    "def increment_array(x):\n",
    "    a = x.copy()\n",
    "    a += 1\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = increment_array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scroll all the way down to the bottom of that error message, you'll see this\n",
    "\n",
    "```\n",
    "LoweringError: Failed at nopython (nopython mode backend)\n",
    "Internal error:\n",
    "NotImplementedError: inplace_binop(rhs=$const0.5, lhs=a, fn=+)\n",
    "File \"<ipython-input-59-de847f90b64c>\", line 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's Numba telling us that there's a problem on line 4 if we want everything to be accelerated.  We can get around this by removing the in-place operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@autojit(nopython=True)\n",
    "def increment_array(x):\n",
    "    a = x.copy()\n",
    "    a = a + 1\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = increment_array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that the function runs and will be accelerated as much as Numba can manage.  Onwards!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<h5>Numba version check</h5>\n",
    "<br>\n",
    "\n",
    "If the \"fixed\" version of `increment_array` still doesn't work then you need to upgrade your version of Numba to at least 0.20.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Back to Jacobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to compare different iterative methods and to do so, we need to make sure we aren't introducing other factors in to these comparisons.  First things first, we want to write a new Jacobi method solver that uses Numba instead of Numpy so we have a proper baseline to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def laplace2d_jacobi(p, pn, y, dx, dy, eps):\n",
    "    \n",
    "    iterations = 0\n",
    "    error = 2*eps\n",
    "    ny, nx = p.shape\n",
    "    l2err = numpy.zeros(30000)\n",
    "    \n",
    "    while error > eps:\n",
    "        for j in range(ny):\n",
    "            for i in range(nx):\n",
    "                pn[j,i] = p[j,i]\n",
    "        error = 0.0\n",
    "        for j in range(1,ny-1):\n",
    "            for i in range(1,nx-1):\n",
    "                p[j,i] = .25 * (pn[j,i-1] + pn[j,i+1] + pn[j-1,i] + pn[j+1,i])\n",
    "                \n",
    "        \n",
    "        #Neumann 2nd-order BC\n",
    "        for j in range(1,ny-1):\n",
    "            p[j,-1] = .25 * (2*pn[j,-2] + pn[j+1,-1] + pn[j-1, -1])\n",
    "            \n",
    "            \n",
    "        for j in range(ny):\n",
    "            for i in range(nx):\n",
    "                error += (p[j,i] - pn[j,i])**2\n",
    "        \n",
    "        \n",
    "        error = numpy.sqrt(error/numpy.sum(pn**2))\n",
    "        l2err[iterations] = error\n",
    "        iterations += 1    \n",
    "        \n",
    "    return p, iterations, l2err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, iterations, l2errJ = laplace2d_jacobi(p0.copy(), p0.copy(), y, dx, dy, 1e-8)\n",
    "\n",
    "print(\"Numba Jacobi method took {} iterations at tolerance {}\".format(iterations, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's actually a little bit slower than the Numpy version of the Jacobi solver, but don't lose heart!  Remember that Numpy is a highly optimized library.  The fact that we're even remotely close to the same execution time with this JIT-compiled code is kind of amazing.  Plus(!) now we get to try out those techniques that aren't possible with Numpy array operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h5>Note</h5>\n",
    "<br>\n",
    "\n",
    "We're also saving the history of how the L2-norm changes over the course of these iterations.  We'll take a look at those once we have a few more methods to compare it to.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Gauss-Seidel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall, the whole reason we got into this Numba sidetrack was to try out Gauss-Seidel.  Recall from above that the formula for Gauss-Seidel is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "p^{n+1}_{i,j} = \\frac{1}{4} \\left(p^{n+1}_{i,j-1} + p^n_{i,j+1} + p^{n+1}_{i-1,j} + p^n_{i+1,j} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "That means we only need to ever so slightly tweak the Jacobi solver to implement Gauss-Seidel.  Instead of updating `p` in terms of `pn`, we just update `p` using `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@autojit(nopython=True)\n",
    "def laplace2d_gauss_seidel(p, pn, y, dx, dy, eps):\n",
    "\n",
    "    iterations = 0\n",
    "    error = 2*eps\n",
    "    ny, nx = p.shape\n",
    "    l2err = numpy.zeros(30000)\n",
    "    \n",
    "    while error > eps:\n",
    "        for j in range(ny):\n",
    "            for i in range(nx):\n",
    "                pn[j,i] = p[j,i]\n",
    "        error = 0.0\n",
    "        for j in range(1,ny-1):\n",
    "            for i in range(1,nx-1):\n",
    "                p[j,i] = .25 * (p[j,i-1] + p[j,i+1] + p[j-1,i] + p[j+1,i])\n",
    "                \n",
    "        \n",
    "        #Neumann 2nd-order BC\n",
    "        for j in range(1,ny-1):\n",
    "            p[j,-1] = .25 * (2*p[j,-2] + p[j+1,-1] + p[j-1, -1])\n",
    "            \n",
    "            \n",
    "        for j in range(ny):\n",
    "            for i in range(nx):\n",
    "                error += (p[j,i] - pn[j,i])**2\n",
    "        \n",
    "        \n",
    "        error = numpy.sqrt(error/numpy.sum(pn**2))\n",
    "        l2err[iterations] = error\n",
    "        iterations += 1    \n",
    "        \n",
    "    return p, iterations, l2err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, iterations, l2errGS = laplace2d_gauss_seidel(p0.copy(), p0.copy(), y, dx, dy, 1e-8)\n",
    "\n",
    "print(\"Numba Gauss-Seidel method took {} iterations at tolerance {}\".format(iterations, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!  Using the most recently updated values saved 6000 iterations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successive Over-Relaxation (SOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successive over-relaxation is an extension of the Gauss-Seidel method.  We take the existing Gauss-Seidel method and use a linear combination of the previous and the current solution to accelerate convergence.  \n",
    "\n",
    "\\begin{equation}\n",
    "p^{n+1}_{i,j} = (1 - \\omega)p^n_{i,j} + \\frac{\\omega}{4} \\left(p^{n+1}_{i,j-1} + p^n_{i,j+1} + p^{n+1}_{i-1,j} + p^n_{i+1,j} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "SOR iterations are only stable for $0 < \\omega < 2$.  \n",
    "\n",
    "If $\\omega < 1$, that is technically an \"under-relaxation\" and it will be slower than Gauss-Seidel.  \n",
    "\n",
    "If $\\omega > 1$, that's the over-relaxation and it should converge faster than Gauss-Seidel.  \n",
    "\n",
    "Also note that for $\\omega = 1$, the equation collapses into the Gauss-Seidel method.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@autojit(nopython=True)\n",
    "def laplace2d_SOR(p, pn, y, dx, dy, eps, omega):\n",
    "    \n",
    "    iterations = 0\n",
    "    error = 2*eps\n",
    "    ny, nx = p.shape\n",
    "    l2err = numpy.zeros(20000)\n",
    "    \n",
    "    while error > eps:\n",
    "        for j in range(ny):\n",
    "            for i in range(nx):\n",
    "                pn[j,i] = p[j,i]\n",
    "        error = 0.0\n",
    "        for j in range(1,ny-1):\n",
    "            for i in range(1,nx-1):\n",
    "                p[j,i] = (1-omega)*p[j,i] + omega*.25 * (p[j,i-1] + p[j,i+1] + p[j-1,i] + p[j+1,i])\n",
    "        \n",
    "        #Neumann 2nd-order BC\n",
    "        for j in range(1,ny-1):\n",
    "            p[j,-1] = .25 * (2*p[j,-2] + p[j+1,-1] + p[j-1, -1])\n",
    "            \n",
    "        for j in range(ny):\n",
    "            for i in range(nx):\n",
    "                error += (p[j,i] - pn[j,i])**2\n",
    "            \n",
    "        error = numpy.sqrt(error/numpy.sum(pn**2))\n",
    "        l2err[iterations] = error\n",
    "        iterations += 1        \n",
    "        \n",
    "    \n",
    "    return p, iterations, l2err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't too bad at all.  Let's try this out first with $\\omega = 1$ and make sure it matches the Gauss-Seidel results from above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "omega = 1\n",
    "p, iterations, l2errSOR = laplace2d_SOR(p0.copy(), p0.copy(), y, dx, dy, eps, omega)\n",
    "\n",
    "print(\"Numba SOR method took {} iterations\\\n",
    " at tolerance {} with omega = {}\".format(iterations, eps, omega))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the exact same number of iterations as Gauss-Seidel.  That's a good sign that things are working as expected.  \n",
    "\n",
    "Now let's try to over-relax the solution and see what happens.  To start, let's try $\\omega = 1.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "omega = 1.5\n",
    "p, iterations, l2errSOR = laplace2d_SOR(p0.copy(), p0.copy(), y, dx, dy, eps, omega)\n",
    "\n",
    "print(\"Numba SOR method took {} iterations\\\n",
    " at tolerance {} with omega = {}\".format(iterations, eps, omega))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow!  That really did the trick!  We dropped from 13939 iterations down to 7108.  Now we're really cooking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned SOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We picked $\\omega=1.5$ somewhat arbitrarily.  Ideally, we would like to over-relax the solution as much as possible without introducing instability, as that will give us the fewest number of iterations.  \n",
    "\n",
    "For square domains, it turns out that the ideal factor $\\omega$ can be computed as a function of the number of nodes in one direction, e.g. `nx`.  \n",
    "\n",
    "\\begin{equation}\n",
    "\\omega \\approx \\frac{2}{1+\\frac{\\pi}{nx}}\n",
    "\\end{equation}\n",
    "\n",
    "This is not some arbitrary formula, but its derivation lies outside the scope of this class.  For now, let's try it out and see how it works.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "omega = 2./(1 + numpy.pi/nx)\n",
    "p, iterations, l2errSOR = laplace2d_SOR(p0.copy(), p0.copy(), y, dx, dy, eps, omega)\n",
    "\n",
    "print(\"Numba SOR method took {} iterations\\\n",
    " at tolerance {} with omega = {:.4f}\".format(iterations, eps, omega))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow!  That's *very* fast.  Also, $\\omega$ is very close to the upper limit of 2.  SOR tends to work fastest when $\\omega$ approaches 2, but don't be tempted to push it.  Set $\\omega = 2$ and the walls will come crumbling down.  \n",
    "\n",
    "How does the L2 error look with only 1110 iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L2_rel_error(p,pan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking very good, indeed.  \n",
    "\n",
    "We didn't explain it in any detail, but notice the very interesting implication of Equation $(5)$: the ideal relaxation factor is a function of the grid size.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##L2 error reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in the [Poisson Equation notebook](02_2D_Poisson_Equation.ipynb) we noted that the steady state check seemed to \"stall out\" as the iterations progressed.  Clearly we have managed to reduce the number of iterations required to reach the desired accuracy -- now we can take a look at what happens to the error in these three methods as the iterations progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(8,8))\n",
    "pyplot.semilogy(numpy.trim_zeros(l2errJ,'b'), label='Jacobi')\n",
    "pyplot.semilogy(numpy.trim_zeros(l2errGS,'b'), label='Gauss-Seidel')\n",
    "pyplot.semilogy(numpy.trim_zeros(l2errSOR,'b'), label='SOR')\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the same sort of behavior we observed in [Lesson 2](./2D_Poisson_Equation.ipynb).  The Jacobi method starts out ok, but then it sort of trails off.  Gauss-Seidel hangs in there a little bit longer, but then it also slows down.  But wow(!), SOR seems to work pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Thomas, James William. Numerical partial differential equations: conservation laws and elliptic equations. Vol. 3. Berlin: Springer, 1999.\n",
    "\n",
    "2.  http://wiki.scipy.org/PerformancePython\n",
    "\n",
    "3.  http://www.physics.buffalo.edu/phy410-505/2011/topic3/app1/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "css_file = '../../styles/numericalmoocstyle.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
