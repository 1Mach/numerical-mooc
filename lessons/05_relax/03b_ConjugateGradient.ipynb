{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under MIT license (c)2014 L.A. Barba, G.F. Forsyth, B. Knaepen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's head in the right direction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we applied the Jacobi method to the resolution of the Poisson equation and how several other methods and techniques can speed up the iterative resolution of elliptic PDEs. However, there is still room for improvement. Before looking at the multigrid technique, let us consider the very popular conjugate gradient (CG) method and apply it to the Poisson equation problem that we solved in Lesson 2. The CG method is either used on its own, or in conjunction with multigrid where the latter is used as a preconditioner (more on that in lesson XX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again consider the Poisson equation\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla^2 p = -2\\left(\\frac{\\pi}{2}\\right)^2\\sin\\left( \\frac{\\pi x}{L} \\right) \\cos\\left(\\frac{\\pi y}{L}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "in the domain \n",
    "\n",
    "$$\\left\\lbrace \\begin{align*}\n",
    "0 &\\leq x\\leq 1  \\\\\n",
    "-0.5 &\\leq y \\leq 0.5 \n",
    "\\end{align*} \\right.$$\n",
    "\n",
    "with boundary conditions \n",
    "\n",
    "$$p=0 \\text{ at } \\left\\lbrace \n",
    "\\begin{align*}\n",
    "x&=0\\\\\n",
    "y&=0\\\\\n",
    "y&=-0.5\\\\\n",
    "y&=0.5\n",
    "\\end{align*} \\right.$$\n",
    "\n",
    "We will solve this equation by assuming an initial state of $p=0$ everywhere, and applying boundary conditions to relax in pseudo-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CG method aims at solving elliptic PDEs by iterating the unknown and having it converge, up to a given accuracy, to the solution dictated by the boundary conditions (it is a member of the so-called Krylov subspace methods, but this should not scare you...).\n",
    "\n",
    "Recall that in its discretized form, the Poisson equation reads,\n",
    "\n",
    "$$\\frac{p_{i+1,j}^{n}-2p_{i,j}^{n}+p_{i-1,j}^{n}}{\\Delta x^2}+\\frac{p_{i,j+1}^{n}-2 p_{i,j}^{n}+p_{i,j-1}^{n}}{\\Delta y^2}=b_{i,j}^{n}$$\n",
    "\n",
    "You may not have immediately noticed, but you certainly manipulated equations of this kind in the past. Indeed, the left hand side represents a linear combination of the values of $p$ at several grid points and this linear combination has to be equal to the value of the source term on the right hand side.\n",
    "\n",
    "Now imagine you gather the values $p_{i,j}$ of $p$ at all grid points into a big vector ${\\bf p}$ and you do the same for $b$ using the same ordering. Both vectors ${\\bf p}$ and ${\\bf b}$ contain $N=nx*ny$ values and thus belong to $\\mathbf{R}^N$. The linear relationship we just described means that the discretised Poisson equation can be symbolically written as,\n",
    "\n",
    "\\begin{equation}\n",
    "A{\\bf p}={\\bf b},\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is a $N\\times N$ matrix. This is nothing other than a linear system of equations in matrix form. Although we will not directly use this form in the actual CG algorithm below, it is useful to examine the problem this way to understand how the method works.\n",
    "\n",
    "The starting point is to define the iteration of the potential $p$ as,\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf p}^{n+1}={\\bf p}^n + \\alpha {\\bf d}^n\n",
    "\\end{equation}\n",
    "\n",
    "At iteration $n+1$, ${\\bf p}^n$ is modified by taking a step in the direction of ${\\bf d}^n$. The idea is to start with a guess ${\\bf p}^0$ and to march towards the solution by making jumps along the direction vectors ${\\bf d}^n$.  $\\alpha$ is a scalar that determines how big a jump to take at each iteration.  Now we need to carefully choose the direction vectors and the size of the jumps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The method of steepest descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before considering the more complex CG algorithm, it is helpful to introduce a simpler approach called the method of steepest descent. At iteration $0$, we choose an inital guess. Unless we are immensely lucky, it will not satisfy the Poisson equation and we will have,\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf b}-A{\\bf p}^0={\\bf r}^0\\ne {\\bf 0}\n",
    "\\end{equation}\n",
    "\n",
    "The vector ${\\bf r}^0$ is called the initial residual and measures how far we are from satisfying the linear system. In fact, we can define such a residual vector at each iteration,\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf r}^n={\\bf b}-A{\\bf p}^n\n",
    "\\end{equation}\n",
    "\n",
    "and eventually monitor how this vector tends to zero as we iterate.\n",
    "\n",
    "In the method of steepest descent, two choices are made:\n",
    "\n",
    "1. the direction vectors are chosen to be the residuals ${\\bf d}^n = {\\bf r}^n$.\n",
    "2. the length of the jumps are such that the $n+1$ residual is orthogonal to the previous one (because of 1., the direction of jump $n+1$ is orthogonal to the one of jump $n$).\n",
    "\n",
    "There are good (and, in fact, not complex) reasons to justify these choices and you should read one of the references supplied if you want to understand them. But since we want you to converge to the end of the notebook in a finite time, please accept them for now. \n",
    "\n",
    "Condition 2 requires that,\n",
    "\n",
    "\\begin{align}\n",
    "{\\bf r}^{n+1}\\cdot {\\bf r}^{n} = 0 \\nonumber \\\\\n",
    "\\Leftrightarrow ({\\bf b}-A{\\bf p}^{n+1}) \\cdot {\\bf r}^{n} = 0 \\nonumber \\\\\n",
    "\\Leftrightarrow ({\\bf b}-A({\\bf p}^{n}+\\alpha {\\bf r}^n)) \\cdot {\\bf r}^{n} = 0 \\nonumber \\\\\n",
    "\\Leftrightarrow ({\\bf r}^n+\\alpha A{\\bf r}^n) \\cdot {\\bf r}^{n} = 0 \\nonumber \\\\\n",
    "\\alpha = \\frac{{\\bf r}^n \\cdot {\\bf r}^n}{A{\\bf r}^n \\cdot {\\bf r}^n}.\n",
    "\\end{align}\n",
    "\n",
    "We are now ready to test this algorithm.\n",
    "\n",
    "To begin, let's import libraries, setup our mesh and import some helper functions as in the previous lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from math import pi\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nx = 41\n",
    "ny = 41\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = -0.5\n",
    "ymax = 0.5\n",
    "\n",
    "\n",
    "l2_target = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from laplace_helper import plot2D, L2_rel_error\n",
    "from cg_helper import poisson_2d, initialization, p_analytical\n",
    "\n",
    "X, Y, x, y, p_i, b, dx, dy, L = initialization(nx, ny, xmax, xmin, ymax, ymin)\n",
    "\n",
    "pan = p_analytical(X,Y,L)\n",
    "plot2D(x,y,pan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1: Analytical solution of the Poisson problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our implementation of steepest descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sd_2d(p, b, dx, dy, l2_target):\n",
    "    '''Performs sd relaxation\n",
    "    Assumes Dirichlet boundary conditions p=0\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    p : 2D array of floats\n",
    "        Initial guess\n",
    "    b : 2D array of floats\n",
    "        Source term\n",
    "    dx: float\n",
    "        Mesh spacing in x direction\n",
    "    dy: float\n",
    "        Mesh spacing in y direction\n",
    "    l2_target: float\n",
    "        Error target\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    p: 2D array of float\n",
    "        Distribution after relaxation\n",
    "    '''\n",
    "    r  = numpy.zeros((ny,nx)) # residual\n",
    "    Ar  = numpy.zeros((ny,nx)) # to store result of matrix multiplication\n",
    "    \n",
    "    l2_norm = 1\n",
    "    iterations = 0\n",
    "    l2_conv = []\n",
    "    \n",
    "    # Iterations\n",
    "    while l2_norm > l2_target:\n",
    "\n",
    "        pd = p.copy()\n",
    "        \n",
    "        r[1:-1,1:-1] = b[1:-1,1:-1]*dx**2 + 4*pd[1:-1,1:-1] - \\\n",
    "            pd[1:-1,2:] - pd[1:-1,:-2] - pd[2:, 1:-1] - pd[:-2, 1:-1]\n",
    "        \n",
    "        Ar[1:-1,1:-1] = -4*r[1:-1,1:-1]+r[1:-1,2:]+r[1:-1,:-2]+\\\n",
    "            r[2:, 1:-1] + r[:-2, 1:-1]\n",
    "\n",
    "        rho = numpy.sum(r*r)\n",
    "        sigma = numpy.sum(r*Ar)\n",
    "        alpha = rho/sigma\n",
    "\n",
    "        p = pd + alpha*r\n",
    "        \n",
    "        # BCs are automatically enforced\n",
    "        \n",
    "        l2_norm = L2_rel_error(pd,p)\n",
    "        iterations += 1\n",
    "        l2_conv.append(l2_norm)\n",
    "    \n",
    "    print('Number of SD iterations: {0:d}'.format(iterations))\n",
    "    return p, l2_conv     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs on our example problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, l2_conv = sd_2d(p_i.copy(), b, dx, dy, l2_target)\n",
    "L2_rel_error(p,pan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Not bad! it took only two iterations to reach a solution that meets our steady state check. Although this seems great, the steepest descent algorithm is not great when used with large systems or more complex right hand sides in the Poisson equation (see challenge below). We can do better by choosing different jump directions as explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The method of conjugate gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With steepest descent, we know that two **successive** jumps are orthogonal, but that's about it.  There is nothing to prevent the algorithm from making several jumps in the same (or a similar) direction.  Imagine you wanted to go from the intersection of the 5th Avenue and 23rd Street to the intersection of 9th Avenue and 30th Street. Knowing that each segment has the same computational cost (one iteration), would you follow the red path or the green path?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/jumps.png\" width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1. Do you take the red path or the green path?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of conjugate gradients is built upon the idea of reducing the number of jumps and makes sure the algorithm never selects the same direction twice. To achieve this, there are two conditions to determine the size of the jumps and the direction vectors:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha = \\frac{{\\bf r}^n \\cdot {\\bf r}^n}{A{\\bf d}^n \\cdot {\\bf d}^n}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf d}^{n+1}={\\bf r}^{n+1}+\\beta{\\bf d}^{n}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta = \\frac{{\\bf r}^{n+1} \\cdot {\\bf r}^{n+1}}{{\\bf r}^n \\cdot {\\bf r}^n}$\n",
    "\n",
    "The search directions are no longer equal to the residuals but are instead a  linear combination of the residual and the previous search direction. It can be shown that this algorithm necessarily converges to the exact solution (up to machine accuracy) in a maximum of $N$ iterations. When one is satisfied with an approximate solution, the number of steps is significantly lower. Again, the derivation of the algorithm is not immensely difficult and can be found in Shewchuk (1994).\n",
    "\n",
    "Here is a function implementing the CG algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cg_2d(p, b, dx, dy, l2_target):\n",
    "    '''Performs cg relaxation\n",
    "    Assumes Dirichlet boundary conditions p=0\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    p : 2D array of floats\n",
    "        Initial guess\n",
    "    b : 2D array of floats\n",
    "        Source term\n",
    "    dx: float\n",
    "        Mesh spacing in x direction\n",
    "    dy: float\n",
    "        Mesh spacing in y direction\n",
    "    l2_target: float\n",
    "        Error target\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    p: 2D array of float\n",
    "        Distribution after relaxation\n",
    "    '''\n",
    "    r  = numpy.zeros((ny,nx)) # residual\n",
    "    Ad  = numpy.zeros((ny,nx)) # to store result of matrix multiplication \n",
    "    \n",
    "    l2_norm = 1\n",
    "    iterations = 0\n",
    "    l2_conv = []\n",
    "    \n",
    "    # Step-0 We compute the initial residual and \n",
    "    # the first search direction is just this residual\n",
    "    \n",
    "    r[1:-1,1:-1] = b[1:-1,1:-1]*dx**2 + 4*p[1:-1,1:-1] - \\\n",
    "        p[1:-1,2:] - p[1:-1,:-2] - p[2:, 1:-1] - p[:-2, 1:-1]\n",
    "    d = r\n",
    "    rho = numpy.sum(r*r)\n",
    "    Ad[1:-1,1:-1] = -4*d[1:-1,1:-1]+d[1:-1,2:]+d[1:-1,:-2]+\\\n",
    "        d[2:, 1:-1] + d[:-2, 1:-1]\n",
    "    sigma = numpy.sum(d*Ad)\n",
    "    \n",
    "    # Iterations\n",
    "    while l2_norm > l2_target:\n",
    "\n",
    "        pd = p.copy()\n",
    "        rd = r.copy()\n",
    "        dd = d.copy()\n",
    "        \n",
    "        alpha = rho/sigma\n",
    "\n",
    "        p = pd + alpha*dd\n",
    "        r = rd - alpha*Ad\n",
    "        \n",
    "        rhop1 = numpy.sum(r*r)\n",
    "        beta = rhop1 / rho\n",
    "        rho = rhop1\n",
    "        \n",
    "        d = r + beta*dd\n",
    "        Ad[1:-1,1:-1] = -4*d[1:-1,1:-1] + d[1:-1,2:] + d[1:-1,:-2] + \\\n",
    "            d[2:, 1:-1] + d[:-2, 1:-1]\n",
    "        sigma = numpy.sum(d*Ad)\n",
    "        \n",
    "        # BCs are automatically enforced\n",
    "        \n",
    "        l2_norm = L2_rel_error(pd,p)\n",
    "        iterations += 1\n",
    "        l2_conv.append(l2_norm)\n",
    "    \n",
    "    print('Number of CG iterations: {0:d}'.format(iterations))\n",
    "    return p, l2_conv     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, l2_conv = cg_2d(p_i.copy(), b, dx, dy, l2_target)\n",
    "L2_rel_error(p,pan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to the number of iterations needed for the Jacobi iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, l2_conv = poisson_2d(p_i.copy(), b, dx, dy, l2_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Shewchuk, J. (1994). An Introduction to the Conjugate Gradient Method Without the Agonizing Pain. URL: http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "css_file = '../../styles/numericalmoocstyle.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
